{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b83de71-675c-4c2c-aef4-b43ddf06cf48",
   "metadata": {},
   "source": [
    "# Everytime you restart container, you do better close the browser and restart JupyterLab UI again to avoid the issues caused by caching!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_table_manager import TableManager\n",
    "from lab_schema_manager import SchemaManager\n",
    "import yfinance as yf\n",
    "from lab_pg_database_manager import PGDatabaseManager\n",
    "from lab_spark import create_spark_session\n",
    "from lab_raw_yfinance import RawYFinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f1668e-6c02-40a4-8d8b-4c1e522420fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_yfinance_record(symbol_date_pairs):\n",
    "    try:\n",
    "        symbol, start_date = symbol_date_pairs\n",
    "        # Fetch stock data using yfinance\n",
    "        quote = yf.Ticker(symbol)\n",
    "        current_date = date.today()\n",
    "        hist = quote.history(start=start_date, end=current_date)\n",
    "\n",
    "        # Reset index to include Date as a column and format it\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist[\"Date\"] = hist[\"Date\"].dt.date\n",
    "        \n",
    "        # limit and stablize the fields of hist\n",
    "        hist = hist[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n",
    "        \n",
    "        # Add symbol and import_time to each row\n",
    "        record_list = [\n",
    "            tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "        # print(record_list)\n",
    "        return record_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return []  # Return an empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel_fetch_yfinance_record(symbol_date_pairs, record_schema):\n",
    "    try:\n",
    "        # Distribute (symbol, start_date) pairs across Spark workers\n",
    "        record_rdd = spark.sparkContext.parallelize(symbol_date_pairs)\n",
    "        \n",
    "        # Fetch data in parallel\n",
    "        mapped_record_rdd = record_rdd.flatMap(fetch_yfinance_record)\n",
    "\n",
    "        # Convert RDD to DataFrame\n",
    "        result_df = spark.createDataFrame(mapped_record_rdd, schema=record_schema)\n",
    "        \n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error paralleling fetch: {e}\")\n",
    "        return spark.createDataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b02da3-a584-4dbd-b44b-ccd3d5bbed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_iceberg_raw_eod_yfinance(symbol_date_pairs, iceberg_sink_table, schema_config_file):\n",
    "    try: \n",
    "        schema_manager=SchemaManager(schema_config_file)\n",
    "        schema_struct_type=schema_manager.get_struct_type(\"tables\", iceberg_sink_table)  \n",
    "        # print(schema_struct_type)\n",
    "        \n",
    "        create_table_script = schema_manager.get_create_table_query(\"tables\", iceberg_sink_table)\n",
    "        # print(create_table_script)\n",
    "        spark.sql(create_table_script)\n",
    "\n",
    "        df_source=parallel_fetch_yfinance_record(symbol_date_pairs, schema_struct_type)        \n",
    "        # df_source.writeTo(iceberg_sink_table).append()\n",
    "        df_source.write.mode(\"overwrite\").saveAsTable(iceberg_sink_table)    \n",
    "\n",
    "        print(f\"{iceberg_sink_table} has been loaded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading lceberg raw table: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2dff869-fe5d-4be5-8762-3ea44b8ce947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pg_finalytics(iceberg_source_table, pg_url, pg_driver, pg_sink_table, pg_truncate_script, pg_merge_script):   \n",
    "    try:     \n",
    "        df_source=spark.read.table(iceberg_source_table)          \n",
    "        finalytics.execute_sql_script(pg_truncate_script)\n",
    "        \n",
    "        # Write DataFrame to PostgreSQL\n",
    "        df_source.write.jdbc(url=pg_url, table=pg_sink_table, mode=\"append\", properties={\"driver\": pg_driver})        \n",
    "        finalytics.execute_sql_script(pg_merge_script)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pg finalytics: {e}\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c3d0f-0086-4ca7-bb71-974aaedc0d9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load nessie.raw.stock_eod_yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ce60ae-b25b-44e2-a2a3-c33c05aa5172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/workspace/finalytics_spark/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f0dd957b-1aeb-4445-8ab9-13fe8012183e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.29.42 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.29.42 in central\n",
      "\tfound software.amazon.awssdk#utils;2.29.42 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.29.42 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.29.42 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.29.42 in central\n",
      ":: resolution report :: resolve 428ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.77.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.29.42 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.29.42 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.29.42 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.29.42 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.29.42 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.29.42 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f0dd957b-1aeb-4445-8ab9-13fe8012183e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/9ms)\n",
      "25/01/03 03:01:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nessie.raw.stock_eod_yfinance has been loaded\n"
     ]
    }
   ],
   "source": [
    "# symbol_start_date_pairs = [\n",
    "#     ('TSLA', '2024-11-20'),\n",
    "#     ('AAPL', '2024-11-20'),\n",
    "#     ('C', '2024-11-20'),\n",
    "# ]\n",
    "\n",
    "# Get finalytics db connetion info\n",
    "conn_config_file='cfg_connections.yaml'\n",
    "finalytics=PGDatabaseManager(conn_config_file, 'finalytics')\n",
    "pg_url=finalytics.jdbc_url\n",
    "pg_driver=finalytics.driver\n",
    "\n",
    "# Get symbol_start_date_pairs from finalytics db\n",
    "query=\"select symbol, start_date from fin.vw_etl_stock_eod_start_date limit 20\"\n",
    "# query=\"select symbol, start_date from fin.vw_etl_stock_eod_start_date\"\n",
    "symbol_start_date_pairs=finalytics.get_sql_script_result_list(query)\n",
    "# print(symbol_start_date_pairs)\n",
    "\n",
    "\n",
    "# Get iceberg table config info\n",
    "table_schema_config_file='cfg_table_schemas.yaml'\n",
    "iceberg_table='nessie.raw.stock_eod_yfinance'\n",
    "\n",
    "# Set global import_time\n",
    "import_time = datetime.now().isoformat()\n",
    "\n",
    "\n",
    "# Create Spark Session\n",
    "connection_config_file=\"cfg_connections.yaml\"\n",
    "spark_app_name=\"raw_yfinance\"\n",
    "spark=create_spark_session(connection_config_file, spark_app_name)\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.raw;\")\n",
    "\n",
    "# Load nessie.raw.stock_eod_yfinance \n",
    "load_iceberg_raw_eod_yfinance(symbol_start_date_pairs, iceberg_table, table_schema_config_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a9dbb-80b6-49fb-8d87-d75e62c60416",
   "metadata": {},
   "source": [
    "## Load finalytics.stage.stock_eod_quote_yahoo and merge into fin.stock_eod_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff6a1d9-c5fa-4697-9b7b-b257048855eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pg_table = \"stage.stock_eod_quote_yahoo\"  # Replace with the PostgreSQL table name\n",
    "\n",
    "pg_truncate_script = f\"truncate table {pg_table};\"\n",
    "\n",
    "pg_merge_script = \"call fin.usp_load_stock_eod();\"\n",
    "\n",
    "load_pg_finalytics(iceberg_table, pg_url, pg_driver, pg_table,  pg_truncate_script, pg_merge_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3904a34-d834-4d28-87b6-21f2b271fbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      37|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f'select count(*) from {iceberg_table}').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06376922-1939-4c55-bde2-0be33707d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+------+-----+--------+---------+------------+------+--------------------------+\n",
      "|date      |open |high |low   |close|volume  |dividends|stock_splits|symbol|import_time               |\n",
      "+----------+-----+-----+------+-----+--------+---------+------------+------+--------------------------+\n",
      "|2024-11-14|1.9  |1.725|1.725 |1.725|129682  |0.0      |0.0         |EIGR  |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|23.54|24.25|23.48 |23.76|956700  |0.0      |0.0         |GDS   |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|23.57|23.85|23.1  |23.18|723400  |0.0      |0.0         |GDS   |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|1.9  |1.91 |1.89  |1.91 |10097100|0.0      |0.0         |BBD   |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|1.87 |2.0  |1.86  |1.96 |63000700|0.0      |0.0         |BBD   |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|3.5  |3.52 |3.26  |3.28 |1220800 |0.0      |0.0         |DDL   |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|3.28 |3.289|3.116 |3.24 |1147600 |0.0      |0.0         |DDL   |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|12.69|12.69|12.69 |12.69|0       |0.0      |0.0         |WTMAU |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|12.69|12.69|12.69 |12.69|0       |0.0      |0.0         |WTMAU |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|3.59 |3.59 |3.36  |3.44 |54100   |0.0      |0.0         |SNT   |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|3.6  |3.69 |3.48  |3.69 |44600   |0.0      |0.0         |SNT   |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|6.93 |7.16 |6.67  |6.72 |27900   |0.0      |0.0         |ASTC  |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|7.06 |7.34 |6.85  |6.85 |3700    |0.0      |0.0         |ASTC  |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|11.79|12.07|11.7  |11.75|27000   |0.0      |0.0         |TWIN  |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|11.88|11.88|11.47 |11.6 |15600   |0.0      |0.0         |TWIN  |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|1.75 |1.81 |1.75  |1.81 |1527    |0.0      |0.0         |SONX  |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|1.83 |1.83 |1.83  |1.83 |662     |0.0      |0.0         |SONX  |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|37.57|38.38|36.88 |37.69|658200  |0.0      |0.0         |VITL  |2025-01-03T03:01:30.433839|\n",
      "|2025-01-02|37.95|39.45|37.895|38.84|444300  |0.0      |0.0         |VITL  |2025-01-03T03:01:30.433839|\n",
      "|2024-12-31|9.19 |9.41 |9.04  |9.17 |185900  |0.0      |0.0         |INGN  |2025-01-03T03:01:30.433839|\n",
      "+----------+-----+-----+------+-----+--------+---------+------------+------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from nessie.raw.stock_eod_yfinance').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a96c0a5-9315-4e5f-a4ed-6c717efd9a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EIGR', '2024-11-15'), ('NYCB', '2024-12-27'), ('GDS', '2024-12-31'), ('BBD', '2024-12-31'), ('DDL', '2024-12-31'), ('WTMAU', '2024-12-31'), ('SNT', '2024-12-31'), ('ASTC', '2024-12-31'), ('TWIN', '2024-12-31'), ('SONX', '2024-12-31'), ('VITL', '2024-12-31'), ('INGN', '2024-12-31'), ('MAQCU', '2024-12-31'), ('GRMN', '2024-12-31'), ('RVLV', '2024-12-31'), ('FVCB', '2024-12-31'), ('ETJ', '2024-12-31'), ('CTSH', '2024-12-31'), ('MMC', '2024-12-31'), ('BBIG', '2024-12-31')]\n"
     ]
    }
   ],
   "source": [
    "# print(symbol_start_date_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fe6e2-70e2-4386-baba-275fdf134a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
