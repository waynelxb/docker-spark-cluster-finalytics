{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71f1ce-97ce-4e88-a374-747acfc540fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import nbimporter\n",
    "from datetime import datetime, date\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,  DateType, TimestampType\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "from lab_spark import create_spark_session\n",
    "import yfinance as yf\n",
    "from lab_pg_database_manager import PGDatabaseManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced30b12-5104-4962-93da-8211e40883bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class raw_finance:\n",
    "    \n",
    "    def __init__(self, spark: SparkSession, equity_type, symbol_date_pairs):\n",
    "        self.spark=spark\n",
    "        self.equity_type=equity_type\n",
    "        self.symbol_date_pairs=symbol_date_pairs\n",
    "\n",
    "        \n",
    "    def fetch_yfinance_record(self):\n",
    "        try:\n",
    "            symbol, start_date = self.symbol_date_pairs\n",
    "            # Fetch stock data using yfinance\n",
    "            quote = yf.Ticker(symbol)\n",
    "            current_date = date.today()\n",
    "            hist = quote.history(start=start_date, end=current_date)\n",
    "    \n",
    "            # Reset index to include Date as a column and format it\n",
    "            hist.reset_index(inplace=True)\n",
    "            hist[\"Date\"] = hist[\"Date\"].dt.date\n",
    "\n",
    "            if self.equity_type = 'stock':\n",
    "                # limit and stablize the fields of hist\n",
    "                hist = hist[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n",
    "                \n",
    "            # Add symbol and import_time to each row\n",
    "            record_list = [\n",
    "                tuple(row) + (symbol, import_time) for row in hist.itertuples(index=False)\n",
    "            ]\n",
    "    \n",
    "            # print(record_list)\n",
    "            return record_list\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []  # Return an empty list on error\n",
    "\n",
    "            \n",
    "   def parallel_fetch_yfinance_record(self, record_schema):\n",
    "        try:\n",
    "            # Distribute (symbol, start_date) pairs across Spark workers\n",
    "            param_rdd = spark.sparkContext.parallelize(self.symbol_date_pairs)    \n",
    "            \n",
    "            # Fetch data in parallel\n",
    "            mapped_record_rdd = param_rdd.flatMap(fetch_yfinance_record)\n",
    "    \n",
    "            # Convert RDD to DataFrame\n",
    "            result_df = self.spark.createDataFrame(mapped_record_rdd, schema=record_schema)\n",
    "            # result_df = spark.createDataFrame(mapped_record_rdd)\n",
    "    \n",
    "            # Show or save the results\n",
    "            # result_df.show()\n",
    "            return result_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error paralleling fetch: {e}\")z\n",
    "            return self.spark.createDataFrame([]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a8628-9f57-478a-9ca5-5e500fceb7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06a9dbb-80b6-49fb-8d87-d75e62c60416",
   "metadata": {},
   "source": [
    "## Load finalytics.stage.stock_eod_quote_yahoo and merge into fin.stock_eod_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6a1d9-c5fa-4697-9b7b-b257048855eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg_table = \"stage.stock_eod_quote_yahoo\"  # Replace with the PostgreSQL table name\n",
    "\n",
    "pg_truncate_script = f\"truncate table {pg_table};\"\n",
    "\n",
    "pg_merge_script = \"call fin.usp_load_stock_eod();\"\n",
    "\n",
    "load_pg_finalytics(iceberg_table, pg_url, pg_driver, pg_table,  pg_truncate_script, pg_merge_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3904a34-d834-4d28-87b6-21f2b271fbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(f'select count(*) from {iceberg_table}').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06376922-1939-4c55-bde2-0be33707d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('select * from nessie.raw.stock_eod_yfinance').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96c0a5-9315-4e5f-a4ed-6c717efd9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(symbol_start_date_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fe6e2-70e2-4386-baba-275fdf134a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
